{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wukFUba_gX7L"
      },
      "outputs": [],
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EUClDS9gu5J",
        "outputId": "80150717-79f1-4e39-e8d7-642025690dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from pyarrow) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEF23Wstgx2f",
        "outputId": "212da55d-376d-4ceb-a662-9f1f3fb6bd8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wikitext_2 = '/content/drive/MyDrive/AI/train-00000-of-00001.parquet'\n",
        "wikitext_103_1 = '/content/drive/MyDrive/AI/train-00001-of-00002.parquet'\n",
        "wikitext_103 = '/content/drive/MyDrive/AI/train-00000-of-00002 (1).parquet'\n",
        "\n",
        "wikitext_2 = pd.read_parquet('/content/drive/MyDrive/AI/train-00000-of-00001.parquet')\n",
        "wikitext_103_1 = pd.read_parquet('/content/drive/MyDrive/AI/train-00001-of-00002.parquet')\n",
        "wikitext_103 = pd.read_parquet('/content/drive/MyDrive/AI/train-00000-of-00002 (1).parquet')\n",
        "\n"
      ],
      "metadata": {
        "id": "aPjUsFe0g0eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikitext_test = '/content/drive/MyDrive/AI/test-00000-of-00001 (2).parquet'\n",
        "wikitext_test = pd.read_parquet('/content/drive/MyDrive/AI/test-00000-of-00001 (2).parquet')"
      ],
      "metadata": {
        "id": "tDSTKNRIg57B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concating all the data a one place"
      ],
      "metadata": {
        "id": "xJD7fOChyw6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_data = pd.concat([wikitext_2, wikitext_103_1, wikitext_103, wikitext_test], ignore_index=True)"
      ],
      "metadata": {
        "id": "RgDC-f8AlhKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_data_train = all_data.sample(n=1000, random_state=42)\n"
      ],
      "metadata": {
        "id": "VOeKxb63BxGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization\n"
      ],
      "metadata": {
        "id": "Sq4jAzeAhXdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Converts text to lowercase and remove non-alphabetic characters such as punctuation, numbers, and etc. Further, it will convert text into number of tokens while removing the stop words using nltk package for stop words and create a vocabulary. ***\n",
        "\n"
      ],
      "metadata": {
        "id": "WDayTYDIy587"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLjLsdimhIIg",
        "outputId": "f74c7446-8499-4a9b-ae1d-b479fc2098aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "all_data_train[\"tokens\"] = all_data_train[\"text\"].apply(custom_tokenize)\n",
        "\n",
        "#create a vocabulary\n",
        "all_tokens = [token for tokens in all_data_train[\"tokens\"] for token in tokens]\n",
        "\n",
        "print(f\"Total number of tokens: {len(all_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw8QCYs4hQJi",
        "outputId": "bf39cc1b-994a-4265-f84d-b0df32a473b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 28896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** This part will store the unique words in vocab using list(set()) and will also add <unknown> token at the 0th index which makes sure that if the mpdel encounters any word that is not present in vocab will be having same vector representation as of <unknown> which can further be considered as one of the consequences of using Word2Vec.***\n"
      ],
      "metadata": {
        "id": "xkTIMrr9z_bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "#get us the most commmon unique words using the Counter\n",
        "word_counts = Counter(all_tokens)\n",
        "\n",
        "#get the unique words in the vocab\n",
        "vocab = list(set(all_tokens))\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")\n",
        "\n",
        "#store the <unknown> at the 0th index\n",
        "vocab = {\"<unknown>\": 0}\n",
        "for i, (word, _) in enumerate(word_counts.most_common(vocab_size - 1), start=1):\n",
        "    vocab[word] = i\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "#reverse methodolgy for getting similar words at the end of the code\n",
        "index2word = {index: word for word, index in vocab.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCF0Rr3njdT_",
        "outputId": "0fd302eb-9def-44c6-872b-2290ddebc10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 10070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Skip-Gram Model"
      ],
      "metadata": {
        "id": "tkCPgvP_tcEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** This section will generate skip-gram pairs with trhe window size of 2 's which means each center word wil be forming 4 unique pairs. If the token or lets say center word is not present in vocab list then it will skip that word and generate for other.***"
      ],
      "metadata": {
        "id": "yIcVcOmU0ul1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "\n",
        "def generate_skip_grams(tokens, vocab):\n",
        "    pairs = []\n",
        "    for i, center_word in enumerate(tokens):\n",
        "        if center_word not in vocab:\n",
        "            continue  # Skip words not in the 1000-word vocab\n",
        "\n",
        "        context_words = tokens[max(0, i-window_size):i] + tokens[i+1:i+window_size+1]\n",
        "        for context_word in context_words:\n",
        "            if context_word in vocab:\n",
        "                pairs.append((center_word, context_word))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# Generate skip-gram pairs for training data\n",
        "skip_grams = []\n",
        "for df in [all_data_train]:\n",
        "    for tokens in df[\"tokens\"]:\n",
        "        skip_grams.extend(generate_skip_grams(tokens, vocab))\n",
        "\n",
        "\n",
        "print(\"Sample Skip-Gram Pairs:\", skip_grams[:10])\n",
        "print(\"Total Pairs:\", len(skip_grams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhr8JjUunRV6",
        "outputId": "6c80a0e1-d1dc-4a0d-a138-745faf4c0440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Skip-Gram Pairs: [('summer', 'olympics'), ('olympics', 'summer'), ('championship', 'standings'), ('championship', 'race'), ('standings', 'championship'), ('standings', 'race'), ('race', 'championship'), ('race', 'standings'), ('smyth', 'report'), ('report', 'smyth')]\n",
            "Total Pairs: 111730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a Custom Dataset"
      ],
      "metadata": {
        "id": "XLdpFv6LvacM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "BAH5psj51YTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** (__init__) method also known as constructor will have the skip-gram pairs consisting of center word and context word. It will aslo map uniwue words to the indices present in the dictionary. (__len__) method will be used to return the number of samples present in the vocab so that Pytorch knows how many samokes are available. (__getitem__) will take index value and will correspond it to the center and context word pair, and if the word is not present it wil pass it to index value 0 which indicates <unknown>. Finally, it will create a DataLoader that will be consisting of custom data with shffle = true which means it will shuffle the training data after each epoch which ensures some sort of randomness.***\n"
      ],
      "metadata": {
        "id": "nCESzKve3cPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, skip_grams, vocab):\n",
        "        self.skip_grams = skip_grams\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.skip_grams)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        center_word, context_word = self.skip_grams[idx]\n",
        "        #0th index indicates the the unknown word whicxh means\n",
        "        #if it encounters any word out of the vocab list then it will have same vector representation as of unknown\n",
        "        center_idx = self.vocab.get(center_word, 0)\n",
        "        context_idx = self.vocab.get(context_word, 0)\n",
        "\n",
        "        return center_idx, context_idx\n",
        "\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "dataset = SkipGramDataset(skip_grams, vocab)\n",
        "\n",
        "#Create a dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "X2pfnLj8vOuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec Model Architecture"
      ],
      "metadata": {
        "id": "CMT6HZ-ryWP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** The model architecture of Word2Vec will be consisting of on ehidden layer where we will apply ReLU activation function and one embedding layer which is also word embedding in Word2Vec. The dimensional space will be 300 which is one of the standard values defined by Google. Also, in Word2Vec model we will be using Softmax function which generate probablities of each word and help us to get words having higher probablity as an output.***"
      ],
      "metadata": {
        "id": "-uaOeFYq1fzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#architecture of the neural network\n",
        "import torch.nn as nn\n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(MyNetwork, self).__init__()\n",
        "        #Embedding layer will covert word index into vector\n",
        "        #the hidden layer will have the ReLU activation function\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, center_word):\n",
        "        embedding_layer = self.embedding(center_word)\n",
        "        hidden_layer = self.linear_relu_stack(embedding_layer)\n",
        "        prob_dist = torch.softmax(hidden_layer, dim=1)\n",
        "\n",
        "        return prob_dist\n"
      ],
      "metadata": {
        "id": "efCllICdyKAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNetwork(vocab_size, embedding_dim=300)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG79LYTiymD3",
        "outputId": "47307ea2-c9e3-452a-cfb2-190aa7206eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNetwork(\n",
            "  (embedding): Embedding(10070, 300)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=300, out_features=300, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=300, out_features=10070, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(f'Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JezJB8lcy1CR",
        "outputId": "2bbec2dd-9036-4ecd-8479-78e29442c495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: embedding.weight | Size: torch.Size([10070, 300]) | Values : tensor([[-4.9781e-01, -7.7608e-01, -6.5185e-01,  1.6786e+00, -1.1290e+00,\n",
            "         -9.9998e-01,  3.1039e-01,  4.6455e-01, -1.1168e+00,  1.1845e+00,\n",
            "          3.7747e-01, -5.0946e-01,  8.8729e-01, -1.7501e+00, -3.3604e-01,\n",
            "         -3.8515e-01, -5.9830e-01,  4.5290e-01,  7.5620e-01, -1.0394e+00,\n",
            "         -3.7295e-01, -4.5421e-01,  9.0775e-02, -4.9140e-01,  1.3058e+00,\n",
            "          4.6818e-01,  7.3596e-01, -8.1127e-01,  6.9989e-01, -5.6058e-02,\n",
            "         -2.0802e-01,  1.3760e+00,  8.7889e-01, -3.1846e-01,  4.9374e-01,\n",
            "          1.1827e+00,  1.4793e+00,  3.7898e-01, -8.4512e-01,  6.2153e-01,\n",
            "          7.5080e-01,  9.3488e-01, -6.8032e-01, -7.7463e-02, -1.1154e+00,\n",
            "         -6.4441e-01, -3.8082e-01,  9.6739e-01, -1.6951e+00,  1.5994e-01,\n",
            "         -4.7873e-01,  3.8184e-01, -8.0511e-01,  4.4277e-01,  1.1747e+00,\n",
            "         -9.4026e-01, -8.8750e-01,  1.8181e-02, -7.3827e-01,  1.1808e-01,\n",
            "          4.7645e-01, -1.8497e+00,  1.0097e+00, -1.5379e-01,  1.7840e-01,\n",
            "          1.4667e+00, -6.9189e-01, -1.0946e+00, -1.7823e+00, -2.0532e+00,\n",
            "         -1.1171e+00,  2.1127e+00, -8.0209e-01, -2.6296e-01,  4.4534e-01,\n",
            "          1.5658e-01,  7.4593e-03, -5.2653e-01,  1.2054e+00, -6.1039e-01,\n",
            "          4.9879e-01,  1.2563e-02, -7.1143e-01,  1.3764e-01, -1.1332e-01,\n",
            "         -2.7169e+00, -3.8965e-01, -9.1200e-01,  1.7679e+00, -1.4791e+00,\n",
            "          7.4047e-01, -1.4476e+00,  4.8441e-01, -3.4472e-01,  1.3507e+00,\n",
            "          6.3077e-01, -4.0105e-01,  6.4530e-02, -4.9104e-01,  4.2053e-01,\n",
            "          2.4900e+00, -1.5753e+00,  7.7147e-01, -1.7471e-01,  6.1955e-02,\n",
            "         -1.1788e+00,  7.5023e-01,  5.1822e-01, -4.7391e-01,  6.3121e-01,\n",
            "         -5.3907e-03, -2.2144e+00, -8.1833e-02, -4.1153e-02, -1.8161e+00,\n",
            "          4.0227e-01, -1.0429e+00, -1.1435e+00,  6.2287e-01, -6.5378e-01,\n",
            "         -3.9266e-01,  2.1640e+00, -1.2105e+00, -6.1932e-01,  1.0459e+00,\n",
            "         -1.0252e-01,  8.5068e-01, -6.0771e-02, -2.7720e-01, -2.2982e-01,\n",
            "         -4.8476e-01, -2.5504e-01,  7.1881e-03, -2.6059e-01, -3.1102e-01,\n",
            "          6.3042e-01, -3.6905e-01,  3.0961e-01,  4.2169e-01,  8.8130e-01,\n",
            "          9.4683e-01, -3.2034e-01, -9.0005e-01,  2.5555e-01, -2.2193e+00,\n",
            "          1.3022e+00, -8.4812e-01,  1.0845e-01,  4.5418e-01,  4.9444e-01,\n",
            "         -5.8449e-01, -5.0538e-02, -7.9677e-01,  1.2246e+00,  1.3445e+00,\n",
            "          1.0051e+00, -1.0190e+00,  7.0027e-01, -1.7409e-01,  3.2414e-01,\n",
            "         -8.8237e-01,  7.1637e-01, -6.7570e-01,  4.5980e-01, -6.3902e-02,\n",
            "          1.6007e+00,  1.1693e+00,  1.3475e+00, -5.4966e-01,  4.8013e-01,\n",
            "          1.1420e+00, -3.5349e-01,  1.0214e+00, -7.5866e-01,  1.5384e+00,\n",
            "         -1.6901e-01,  3.8821e-01,  1.5048e+00, -7.2143e-01,  2.5268e-01,\n",
            "         -4.2123e-03, -1.2528e+00, -7.8970e-01, -1.8816e+00, -1.1042e+00,\n",
            "         -8.4079e-01,  9.0960e-01, -8.0058e-01,  7.5400e-01, -1.0292e-02,\n",
            "         -6.7643e-01, -3.7522e+00,  1.2896e+00, -6.7500e-02,  1.1870e+00,\n",
            "          2.6164e+00, -5.4570e-01,  3.8873e-01,  8.4558e-01,  1.2413e+00,\n",
            "         -7.6924e-01,  7.0528e-01, -2.2269e-01,  7.0280e-01,  9.3689e-01,\n",
            "          1.0982e-01, -7.9900e-01, -6.6663e-01, -1.8259e-01, -4.2845e-01,\n",
            "          5.4444e-01,  1.0024e+00, -8.0576e-01,  1.1509e-02, -6.6227e-01,\n",
            "          6.3698e-01,  1.3778e+00, -8.0910e-01, -7.4781e-01,  1.5390e+00,\n",
            "          1.0299e+00,  2.6894e-01,  8.2050e-01,  4.8888e-01, -4.6350e-01,\n",
            "         -3.2800e-01, -1.1817e+00, -1.4454e+00,  3.8464e-01, -1.0106e+00,\n",
            "         -4.7302e-02, -2.0281e+00,  5.8394e-01,  2.4311e-02,  2.4937e+00,\n",
            "          2.2790e+00,  8.8889e-02,  5.9260e-03,  1.6190e-01,  4.0410e-01,\n",
            "         -9.4581e-01, -1.5797e+00, -1.9641e+00,  1.7803e-02, -1.5208e+00,\n",
            "          4.0357e-01, -7.0649e-01, -9.9013e-01,  1.6090e+00,  2.1728e+00,\n",
            "          5.2636e-01,  1.7395e+00,  1.6178e+00,  5.9480e-01, -1.6316e+00,\n",
            "          9.4324e-01,  1.8668e-02,  1.6446e+00,  7.8505e-01,  2.0260e+00,\n",
            "          1.6449e+00, -3.5229e-02, -2.0001e+00, -1.6558e+00, -3.8135e-01,\n",
            "         -8.9290e-01, -4.4762e-02, -8.2734e-01, -1.5141e-01,  5.6030e-01,\n",
            "         -9.0624e-01, -9.4708e-01, -4.7453e-01,  9.5495e-01,  3.2916e-01,\n",
            "         -1.5724e+00,  9.1918e-01,  5.1268e-03, -1.0705e-01,  4.5048e-01,\n",
            "         -5.7110e-01,  9.9082e-01, -4.7025e-01,  1.8221e-01,  1.9934e-01,\n",
            "          1.4542e+00, -1.1333e-01, -2.0437e-01, -5.6344e-01,  3.1996e-02,\n",
            "         -4.8588e-01, -1.0861e+00, -4.5656e-01, -1.4213e+00,  8.4878e-01,\n",
            "          5.2798e-01, -3.5750e+00, -1.6189e+00,  7.8147e-01,  3.2697e-01],\n",
            "        [ 2.0542e-01,  2.3912e+00,  3.9768e-01, -8.9455e-02,  1.5949e+00,\n",
            "          8.1899e-02, -6.6981e-01,  2.4059e-01, -6.1318e-01, -4.8984e-01,\n",
            "         -2.0380e+00, -3.1556e-01,  1.0055e+00, -7.1450e-01,  5.3589e-01,\n",
            "          1.5360e-02, -1.1744e+00, -6.1744e-01, -7.3242e-01, -1.7157e-01,\n",
            "          1.4626e+00, -1.2575e+00, -8.5638e-01,  2.6784e-02,  1.6662e+00,\n",
            "         -1.8570e-01,  3.9525e-01, -1.7327e-01,  8.3813e-01, -8.9760e-01,\n",
            "          1.7931e+00,  5.6634e-02,  9.5680e-01, -7.9079e-01, -8.0604e-01,\n",
            "          8.9848e-01, -1.3247e+00,  5.6905e-01,  1.5761e+00, -6.9674e-01,\n",
            "          7.7448e-01,  1.6774e-01,  8.6290e-01, -8.7663e-01,  2.7711e+00,\n",
            "         -2.3782e-01, -1.5756e+00, -5.8500e-02, -1.4257e-02,  9.1544e-01,\n",
            "          1.0150e+00, -5.6001e-01, -1.6199e-01,  1.7317e+00,  1.1359e+00,\n",
            "         -1.2515e-02, -5.3723e-01,  1.1872e+00, -3.1908e-01, -5.5177e-01,\n",
            "          6.8882e-01, -4.7542e-01, -3.3347e-02,  3.4955e-01,  8.3111e-01,\n",
            "          8.5262e-01,  3.0173e-01,  9.7712e-01,  4.9019e-01, -3.7975e-01,\n",
            "          7.9321e-01,  1.4327e+00, -6.0012e-01,  9.2350e-01,  9.6282e-01,\n",
            "          5.4813e-01,  2.4724e-01,  1.6488e-01, -1.9884e-01,  2.5841e-01,\n",
            "         -8.1132e-01,  1.0502e+00,  4.7061e-01, -4.2302e-01,  9.0308e-01,\n",
            "         -2.8752e-02,  8.4944e-01,  1.0042e+00,  1.0069e+00, -1.0378e+00,\n",
            "          6.3840e-01,  8.0785e-01,  8.5481e-01, -8.2396e-01,  1.5775e+00,\n",
            "         -7.1091e-01, -1.3767e+00, -4.5701e-02, -1.6500e+00,  3.0441e-01,\n",
            "          8.1752e-02, -8.5278e-01, -9.5798e-01,  1.3264e-02, -1.4854e+00,\n",
            "         -1.7587e-01,  9.8338e-01, -9.1135e-01, -1.7329e+00,  4.3346e-01,\n",
            "         -2.4997e-01, -3.8915e-01, -1.0380e-01, -1.0330e-01, -2.9917e+00,\n",
            "          3.2578e-01,  6.2523e-01, -7.2140e-01, -1.2396e+00, -5.7170e-02,\n",
            "          1.3156e-01, -2.9942e-01,  5.2120e-01,  1.1393e+00, -7.9691e-01,\n",
            "          1.4625e-01, -1.9675e-01, -7.7599e-01, -5.4572e-01,  1.4385e+00,\n",
            "          8.8084e-01,  5.1078e-02, -1.6043e+00, -7.8154e-01,  7.0181e-01,\n",
            "          2.0466e-01,  7.5173e-01, -8.3900e-01,  6.1908e-01, -5.2071e-01,\n",
            "         -1.5257e+00,  1.2900e-01,  1.6610e+00,  3.6697e-01, -1.2316e-01,\n",
            "         -1.0884e+00, -8.4119e-01, -1.6739e+00,  5.9645e-01,  6.9998e-01,\n",
            "         -6.1594e-01, -3.1875e-01, -1.1313e+00, -7.2162e-02,  4.7558e-01,\n",
            "          8.7079e-01,  2.6149e+00, -2.5469e-01, -8.7962e-01,  6.1255e-01,\n",
            "          3.6737e-01,  2.7979e-01,  6.8787e-01,  9.1968e-01,  1.1182e+00,\n",
            "         -4.7426e-01, -6.0524e-01, -1.5044e+00,  8.2422e-01,  8.5952e-01,\n",
            "         -9.7214e-01,  2.0230e+00,  1.3085e-01, -7.4139e-01,  2.1730e-01,\n",
            "          7.2547e-01,  1.6806e-01,  4.0911e-01,  1.9354e-02,  6.8696e-02,\n",
            "          6.7932e-03,  1.4079e+00, -3.6896e-01,  2.0837e-01, -1.4233e+00,\n",
            "          1.2867e+00, -2.3992e+00, -1.1118e-01,  8.6547e-01,  1.4508e-01,\n",
            "         -9.5664e-02,  7.9800e-01, -3.7956e+00,  1.0596e+00, -2.2805e+00,\n",
            "         -2.5009e-01,  2.0318e+00, -7.3362e-01, -4.3754e-01,  1.2778e+00,\n",
            "         -1.1039e+00,  2.6306e-01,  2.1200e+00,  1.5079e+00, -4.8471e-02,\n",
            "         -5.2402e-01, -1.2734e+00,  1.7635e-01,  3.8296e-01, -1.4744e+00,\n",
            "          2.6012e-01,  1.2079e+00,  1.3374e+00,  8.5884e-02, -3.5446e-01,\n",
            "          5.9068e-01, -4.9490e-01, -2.2561e+00,  1.4836e+00, -4.1514e-01,\n",
            "          8.0187e-04,  2.1094e-01, -2.4685e-01,  9.5216e-01, -1.1216e+00,\n",
            "          3.5509e-01, -4.0773e-01, -3.5043e-03,  3.7324e-01,  9.7841e-01,\n",
            "         -8.0661e-01, -5.1856e-01,  7.8766e-01,  4.5147e-01,  9.4428e-01,\n",
            "          7.6837e-01,  2.5121e+00,  1.3064e+00, -1.2549e+00, -1.2922e+00,\n",
            "          6.0351e-01,  9.2846e-01,  2.6022e-01, -7.2080e-01,  1.5487e+00,\n",
            "          6.0256e-01, -2.7773e-01, -1.5969e+00,  1.2821e+00, -1.1070e+00,\n",
            "          1.9601e+00,  2.0086e+00,  5.4055e-01, -1.7131e+00,  1.0546e+00,\n",
            "         -1.8868e-01, -4.5811e-01,  1.5052e+00, -9.5873e-01,  5.7465e-01,\n",
            "         -2.1951e+00, -3.2626e-02, -9.9154e-01, -6.6713e-01, -1.0344e+00,\n",
            "          3.4351e-01,  1.6780e+00,  1.6291e+00,  4.9546e-01, -5.9243e-01,\n",
            "          1.3794e+00, -5.8956e-03, -1.2281e+00, -9.4928e-01,  1.8190e-01,\n",
            "          5.2674e-01,  1.1182e+00, -7.2680e-01, -7.8667e-02, -1.3340e+00,\n",
            "         -5.4727e-01,  2.4096e-01,  1.4882e+00, -3.4340e-01,  1.1836e+00,\n",
            "          2.3299e-01, -6.1259e-01, -1.2291e+00,  1.5402e+00,  2.1959e+00,\n",
            "         -8.5351e-01, -1.8830e-01, -4.5930e-01, -1.2485e+00, -5.8549e-01,\n",
            "          2.3847e-01, -3.3929e-01,  1.2864e+00, -1.8930e+00, -1.1221e+00]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([300, 300]) | Values : tensor([[ 3.4028e-02, -4.6301e-02, -2.1523e-03,  4.2730e-02, -3.2382e-02,\n",
            "         -1.7755e-02,  2.2240e-03,  3.1305e-02, -2.3875e-03,  3.8841e-02,\n",
            "         -5.6177e-02,  2.7268e-02, -5.4830e-02,  2.7781e-02,  3.3646e-02,\n",
            "          2.7133e-02, -1.7835e-02, -4.5833e-02,  3.7786e-02,  2.1831e-02,\n",
            "         -3.9671e-02,  3.4393e-02, -2.4342e-02,  2.0702e-02, -1.6600e-03,\n",
            "          5.6359e-02,  1.5325e-03,  3.1238e-02, -1.1792e-02,  1.4559e-02,\n",
            "          2.0427e-02, -1.7414e-02,  2.6372e-02, -4.0852e-02,  2.6694e-02,\n",
            "          4.9708e-02,  3.7458e-02,  4.4886e-02, -4.0041e-03, -5.6516e-02,\n",
            "          4.6251e-02,  3.4420e-02,  5.2385e-02,  1.4417e-02,  1.8038e-02,\n",
            "          2.5098e-02,  5.6020e-02, -5.0207e-02,  3.3062e-02,  2.5157e-02,\n",
            "         -3.4080e-02,  4.6257e-02,  1.5848e-04,  3.8524e-02, -5.4135e-03,\n",
            "         -3.7365e-02,  4.4425e-02,  3.7826e-02,  3.7574e-02, -1.7650e-02,\n",
            "         -2.0686e-02,  3.5495e-02, -4.1472e-02, -5.3532e-02, -4.6555e-02,\n",
            "         -4.1516e-02, -2.8975e-02,  4.3434e-02,  5.7251e-02,  1.4460e-02,\n",
            "          5.6101e-02, -1.0655e-04, -1.1828e-03,  5.5096e-02,  1.5812e-02,\n",
            "          4.8571e-02,  2.0846e-02,  1.9272e-02, -3.1128e-03, -4.9613e-02,\n",
            "          3.6488e-02,  3.7691e-02,  4.0681e-02, -4.1163e-02,  2.4503e-02,\n",
            "         -4.5779e-02, -4.0957e-02, -3.9848e-02, -3.3108e-02, -3.4413e-03,\n",
            "          1.1151e-03,  3.1302e-02,  3.9632e-02, -5.2679e-02,  4.9008e-02,\n",
            "         -5.0475e-02,  2.9273e-02, -3.1931e-02, -4.4991e-02,  5.7553e-03,\n",
            "         -3.2123e-02, -2.6169e-02, -4.9893e-02,  2.6798e-03,  4.3636e-02,\n",
            "          4.8197e-02, -4.5607e-02, -4.8512e-02,  3.4549e-02,  1.3503e-02,\n",
            "          2.5499e-02, -4.7318e-02, -3.6226e-02,  7.7883e-03, -6.0456e-03,\n",
            "         -3.3031e-02, -5.0710e-02,  4.1704e-02,  5.6905e-02, -1.6301e-02,\n",
            "         -4.9685e-02,  1.0070e-02,  4.7667e-02,  4.3354e-02, -2.8491e-02,\n",
            "         -4.4810e-02, -2.3295e-02, -3.5833e-02, -3.9823e-02, -2.0665e-02,\n",
            "          9.3729e-04, -4.6584e-02,  1.6601e-02, -4.3220e-02,  4.1349e-02,\n",
            "          1.0840e-02,  4.2910e-02, -4.3476e-02, -2.7415e-02, -1.7959e-02,\n",
            "         -3.4400e-02,  5.0248e-02, -1.6135e-02, -8.8730e-04, -4.1065e-02,\n",
            "          2.9337e-02, -3.9113e-02,  4.9181e-02,  5.4446e-02, -2.2775e-02,\n",
            "         -3.6680e-02,  5.2087e-02, -5.1836e-03,  2.7253e-02, -5.3923e-02,\n",
            "         -1.7164e-02, -2.9339e-02, -4.6640e-02,  5.7146e-02, -4.2584e-02,\n",
            "          9.2514e-03, -1.6751e-02, -4.4153e-02,  4.8488e-02,  2.8533e-02,\n",
            "         -1.8940e-02, -3.7791e-04,  3.1667e-03, -5.3118e-02,  4.8386e-02,\n",
            "         -2.0027e-02, -3.5918e-02,  4.0884e-02,  6.0206e-03,  4.9870e-02,\n",
            "          5.6234e-02,  4.2013e-02,  5.0846e-02,  9.1239e-03,  4.7637e-02,\n",
            "          1.5724e-02,  7.8053e-03,  5.0200e-02, -5.6229e-02,  2.2471e-02,\n",
            "          5.6330e-02,  2.1222e-02,  2.4517e-02,  6.1649e-03, -3.2300e-02,\n",
            "         -3.4436e-02, -1.2967e-02, -5.7628e-02, -2.2614e-02,  4.9227e-02,\n",
            "         -2.8407e-02, -3.1415e-02, -4.7951e-02,  7.9084e-03,  5.3627e-02,\n",
            "         -1.2305e-02,  3.6804e-02, -3.5081e-02,  1.5498e-02,  2.4910e-02,\n",
            "          5.2741e-02,  8.6238e-03,  2.5032e-02, -3.7750e-03, -1.7158e-02,\n",
            "         -1.2617e-03, -9.3822e-03,  1.7661e-02,  4.7145e-02, -3.4945e-02,\n",
            "         -5.5754e-02, -5.0958e-03, -1.8056e-02,  2.8995e-02,  1.9609e-02,\n",
            "         -5.2446e-02,  1.4786e-02,  5.1675e-02,  1.0701e-02,  4.4469e-03,\n",
            "          1.6376e-02, -5.3274e-02,  2.0354e-02, -2.1758e-02, -1.6303e-02,\n",
            "          1.6974e-02,  7.9394e-03,  5.3441e-02,  5.4700e-02, -9.2563e-03,\n",
            "          3.6722e-02,  4.1475e-02,  5.6687e-02, -4.2238e-02, -4.2701e-02,\n",
            "          4.9077e-02,  2.8429e-02, -3.5455e-02,  3.5214e-02,  3.2454e-02,\n",
            "          3.8872e-02, -5.6650e-03, -9.7743e-03,  1.8666e-02,  5.3734e-03,\n",
            "         -2.9760e-02, -4.9419e-02,  3.1074e-02, -2.6659e-02,  2.6269e-03,\n",
            "         -3.3587e-02, -3.1227e-03, -4.4243e-02, -2.1917e-02, -2.5935e-02,\n",
            "          5.2821e-02, -4.7527e-02, -4.2467e-02, -3.2653e-02,  2.4853e-02,\n",
            "          1.4312e-02, -5.3332e-02,  4.8929e-02,  2.8655e-02, -2.6094e-02,\n",
            "         -6.4354e-03, -2.0412e-03,  3.9236e-02, -1.0222e-02,  5.6178e-02,\n",
            "          4.1844e-02, -2.2294e-03, -3.1545e-02,  1.8537e-02, -1.5039e-02,\n",
            "          1.1226e-02, -4.9108e-02, -4.4254e-02, -5.0948e-02, -1.6086e-02,\n",
            "         -3.1023e-02,  4.2669e-02,  5.6946e-03,  2.5503e-02,  5.6528e-02,\n",
            "          3.7655e-02, -4.0787e-02, -4.8582e-02,  4.4800e-02, -3.1379e-02,\n",
            "         -1.2166e-02,  1.4101e-02,  5.0046e-02, -3.3479e-02, -1.8943e-02],\n",
            "        [-2.1761e-02,  3.2992e-02,  2.0512e-02,  1.7466e-02, -4.2982e-03,\n",
            "          3.4243e-02, -2.8600e-02, -2.5354e-02, -5.4193e-02, -4.2791e-02,\n",
            "          4.5547e-04, -3.4715e-02, -4.3275e-02, -3.7686e-02, -2.5043e-02,\n",
            "         -1.4807e-02,  1.5203e-02, -4.7389e-02,  1.9318e-02, -3.1643e-02,\n",
            "         -4.9288e-02, -2.9851e-03,  2.5748e-02, -1.2357e-02,  5.6346e-02,\n",
            "          1.4389e-02,  1.4292e-02, -3.3432e-02, -3.3311e-02, -4.3020e-02,\n",
            "         -1.1374e-02, -2.5372e-02, -5.5884e-02, -4.7138e-02,  4.2193e-02,\n",
            "         -2.8125e-02, -3.2043e-02, -1.6339e-02,  4.2531e-02,  2.7986e-02,\n",
            "         -1.4260e-02,  4.3510e-02,  4.6933e-02,  2.6571e-02,  1.5014e-02,\n",
            "          1.9466e-02, -2.2002e-02,  2.9487e-02, -2.2648e-02,  3.8545e-02,\n",
            "         -3.2646e-02,  9.5766e-03, -4.2568e-03,  3.2825e-02, -2.9833e-02,\n",
            "          3.7336e-02,  1.4934e-02, -3.7470e-02, -2.5157e-02,  6.4154e-03,\n",
            "         -2.5581e-02,  4.5254e-02, -2.1346e-02, -5.4972e-02,  3.2433e-02,\n",
            "         -5.7406e-03,  3.8153e-02, -1.1926e-02, -4.4074e-02,  1.9668e-02,\n",
            "         -3.1728e-02, -3.1485e-02,  1.3092e-02, -1.7503e-02, -3.2776e-02,\n",
            "          1.9520e-03, -1.6748e-02, -5.7195e-02,  6.2084e-03,  2.5776e-02,\n",
            "         -6.5943e-03, -2.2088e-02,  5.6741e-02,  3.9671e-02,  2.2331e-02,\n",
            "         -4.4747e-02,  3.4412e-02,  1.4936e-02, -7.8839e-03,  3.0607e-02,\n",
            "          1.5775e-02, -5.5189e-02,  4.9130e-02,  2.2143e-02, -3.5875e-02,\n",
            "         -4.5229e-02, -1.2848e-02, -1.6315e-02,  2.2435e-02, -2.1225e-02,\n",
            "          2.1631e-02,  2.5902e-02,  2.1103e-02, -1.9458e-02,  2.3578e-02,\n",
            "         -4.1331e-02,  5.1700e-02, -2.3914e-02,  3.7265e-02,  1.4044e-02,\n",
            "          3.4345e-02, -4.0948e-02,  2.6897e-02,  5.5064e-02,  3.3643e-02,\n",
            "          4.0871e-02, -5.1296e-02, -3.2728e-03,  1.8232e-02,  5.5038e-02,\n",
            "          1.0716e-02,  2.1512e-02, -6.6989e-03, -1.6023e-03,  4.7329e-02,\n",
            "         -1.6080e-02, -5.6863e-02, -3.2549e-03, -3.4245e-02, -4.5144e-02,\n",
            "         -1.3170e-02,  1.4374e-02, -3.6184e-02, -2.7092e-02,  5.3309e-02,\n",
            "         -1.7738e-02, -5.0232e-03, -4.0468e-02,  5.4616e-02,  4.3034e-02,\n",
            "         -2.8716e-02,  5.6394e-02,  8.1634e-05,  5.0611e-02, -3.1424e-02,\n",
            "          3.9467e-02, -1.8356e-03,  9.1400e-03, -3.5315e-02, -2.1586e-02,\n",
            "          4.1229e-02,  3.0629e-02,  3.8359e-03,  4.5606e-02,  4.3806e-02,\n",
            "          4.5352e-03,  2.8268e-02,  1.5770e-02,  1.4056e-02,  5.2497e-02,\n",
            "         -2.5533e-02, -3.8424e-02,  4.0328e-02,  4.7433e-02, -1.3958e-02,\n",
            "         -2.1731e-02, -4.6997e-02,  3.5774e-02,  4.8365e-03, -1.6355e-02,\n",
            "          1.9143e-02, -4.9089e-04, -5.2269e-02, -1.8025e-02,  8.8936e-03,\n",
            "         -4.7664e-02, -1.9125e-02, -2.8926e-02, -2.8947e-02, -1.0156e-02,\n",
            "          3.0301e-02, -1.9686e-02, -3.3973e-02, -2.1689e-02, -3.5003e-02,\n",
            "          6.9956e-03,  3.4155e-02, -4.1151e-02,  2.6097e-02, -4.4071e-02,\n",
            "         -4.9373e-02, -4.8794e-02,  5.1360e-02,  5.7709e-02, -1.0010e-02,\n",
            "         -2.1508e-02,  5.3307e-02,  3.6724e-02, -1.3066e-02,  5.2121e-02,\n",
            "         -6.5268e-04, -2.5017e-02, -3.1034e-03,  3.0713e-02,  3.5058e-02,\n",
            "         -5.4148e-02,  4.5074e-02, -3.1440e-02,  4.0115e-04, -3.4523e-02,\n",
            "         -5.7894e-03, -4.4042e-03, -4.8339e-02, -4.8636e-02, -3.8096e-02,\n",
            "          2.1129e-02, -4.5692e-02,  5.3678e-02, -5.3405e-02, -2.6217e-02,\n",
            "          5.7665e-02, -3.3771e-02,  4.8192e-02,  2.2874e-02,  4.6841e-02,\n",
            "          5.8341e-03,  3.2841e-03, -1.2345e-02,  1.1879e-02,  5.1269e-02,\n",
            "          1.4593e-02, -3.7991e-02, -4.5184e-02, -2.9885e-02,  1.8426e-02,\n",
            "          5.5947e-02, -7.4016e-04, -2.1190e-02,  1.5497e-02,  2.8098e-02,\n",
            "         -2.3660e-03, -4.4096e-02, -2.9587e-02,  3.5511e-03, -2.7622e-02,\n",
            "         -1.7894e-02,  1.5617e-02,  4.7587e-02,  3.0299e-02, -2.2282e-02,\n",
            "         -2.9260e-02, -3.3551e-02, -1.7966e-02, -3.4764e-02, -3.8682e-02,\n",
            "         -9.9003e-04, -1.2542e-02, -5.2078e-02, -3.5870e-02, -3.7764e-02,\n",
            "         -5.5995e-02,  1.6541e-03, -5.0173e-03,  1.1050e-02, -2.6771e-02,\n",
            "          1.5850e-02,  2.7333e-02, -4.0700e-02, -3.1071e-02, -1.2877e-02,\n",
            "          6.2291e-03,  4.8795e-02,  2.0122e-02, -4.7072e-02,  1.4833e-02,\n",
            "          4.8410e-02, -4.1669e-02, -2.6802e-02, -2.0290e-02,  1.1180e-02,\n",
            "         -4.2210e-02,  5.4130e-02,  5.5891e-03, -1.6733e-02, -1.2291e-02,\n",
            "          4.7095e-03,  1.0203e-02, -5.2052e-02,  1.2628e-02, -3.4030e-02,\n",
            "         -1.1727e-02,  1.5580e-02,  5.6680e-02, -1.8538e-02,  4.7323e-02,\n",
            "          3.7553e-02,  4.5374e-02,  9.9457e-03,  2.4368e-02, -7.7423e-04]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([300]) | Values : tensor([ 0.0552, -0.0183], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([10070, 300]) | Values : tensor([[ 0.0097, -0.0032, -0.0184, -0.0043, -0.0168,  0.0397, -0.0088,  0.0209,\n",
            "         -0.0178, -0.0102,  0.0527,  0.0155,  0.0455, -0.0231,  0.0419,  0.0297,\n",
            "          0.0073, -0.0069, -0.0105,  0.0445, -0.0326,  0.0268,  0.0323, -0.0087,\n",
            "         -0.0373, -0.0248,  0.0270,  0.0287, -0.0065,  0.0460,  0.0093, -0.0547,\n",
            "         -0.0150,  0.0125, -0.0469, -0.0319,  0.0224,  0.0154, -0.0266, -0.0381,\n",
            "          0.0491,  0.0176,  0.0037,  0.0473,  0.0081, -0.0233, -0.0237,  0.0045,\n",
            "         -0.0126,  0.0323,  0.0550, -0.0129,  0.0282,  0.0011, -0.0155, -0.0348,\n",
            "         -0.0324,  0.0068,  0.0504,  0.0483,  0.0299, -0.0268, -0.0047,  0.0446,\n",
            "         -0.0186,  0.0022, -0.0310,  0.0190,  0.0308, -0.0559, -0.0568, -0.0119,\n",
            "         -0.0131, -0.0528,  0.0217, -0.0108,  0.0170, -0.0398, -0.0536,  0.0331,\n",
            "         -0.0560, -0.0487,  0.0200, -0.0263, -0.0393,  0.0055, -0.0221,  0.0183,\n",
            "          0.0364,  0.0300,  0.0559,  0.0483,  0.0104, -0.0409,  0.0574,  0.0564,\n",
            "         -0.0074,  0.0027,  0.0267,  0.0074, -0.0042, -0.0054, -0.0264, -0.0231,\n",
            "         -0.0028,  0.0273, -0.0205, -0.0120,  0.0513, -0.0161, -0.0120, -0.0561,\n",
            "          0.0178, -0.0237,  0.0302,  0.0212, -0.0157,  0.0542, -0.0141, -0.0026,\n",
            "          0.0138,  0.0254, -0.0555, -0.0228,  0.0159,  0.0477, -0.0156,  0.0412,\n",
            "         -0.0485,  0.0231,  0.0465, -0.0341,  0.0296, -0.0111, -0.0553, -0.0152,\n",
            "         -0.0360, -0.0325, -0.0059, -0.0191,  0.0315, -0.0115, -0.0503,  0.0488,\n",
            "          0.0373,  0.0455, -0.0148, -0.0028,  0.0082, -0.0002, -0.0127, -0.0055,\n",
            "         -0.0201,  0.0337,  0.0122, -0.0257, -0.0329, -0.0204, -0.0408,  0.0376,\n",
            "          0.0281, -0.0093, -0.0398,  0.0566, -0.0132,  0.0036,  0.0402,  0.0468,\n",
            "          0.0004,  0.0221,  0.0502,  0.0041,  0.0028, -0.0475, -0.0432, -0.0365,\n",
            "         -0.0061, -0.0171,  0.0382,  0.0116, -0.0020,  0.0142, -0.0262, -0.0402,\n",
            "          0.0116,  0.0295, -0.0303,  0.0337,  0.0487, -0.0133, -0.0108,  0.0540,\n",
            "         -0.0480, -0.0198, -0.0513,  0.0289, -0.0315,  0.0074,  0.0423,  0.0455,\n",
            "         -0.0285,  0.0042, -0.0455, -0.0190, -0.0343, -0.0081, -0.0050, -0.0122,\n",
            "         -0.0287,  0.0456,  0.0276,  0.0346, -0.0306,  0.0378,  0.0324,  0.0370,\n",
            "          0.0148, -0.0165,  0.0432, -0.0563,  0.0102, -0.0545,  0.0254, -0.0005,\n",
            "          0.0335, -0.0377, -0.0529, -0.0158,  0.0315, -0.0096, -0.0441, -0.0491,\n",
            "          0.0268,  0.0003, -0.0545,  0.0119, -0.0225, -0.0452, -0.0506,  0.0558,\n",
            "          0.0362,  0.0363, -0.0340,  0.0343,  0.0534, -0.0405, -0.0028, -0.0295,\n",
            "         -0.0340, -0.0110, -0.0377, -0.0167,  0.0540, -0.0446, -0.0256,  0.0060,\n",
            "          0.0434, -0.0022,  0.0538, -0.0474,  0.0220, -0.0117,  0.0452,  0.0010,\n",
            "          0.0533, -0.0452,  0.0019,  0.0354, -0.0444,  0.0329, -0.0199, -0.0451,\n",
            "         -0.0140, -0.0403, -0.0107, -0.0300, -0.0105, -0.0320,  0.0503,  0.0306,\n",
            "          0.0110, -0.0408, -0.0369, -0.0387,  0.0321, -0.0354, -0.0561,  0.0116,\n",
            "         -0.0113, -0.0088,  0.0311,  0.0433, -0.0451, -0.0268, -0.0358,  0.0565,\n",
            "          0.0030,  0.0037,  0.0413,  0.0496],\n",
            "        [-0.0319,  0.0548, -0.0427, -0.0084, -0.0085, -0.0149,  0.0097, -0.0288,\n",
            "         -0.0141,  0.0544, -0.0007, -0.0219,  0.0447,  0.0274,  0.0456, -0.0270,\n",
            "          0.0154,  0.0242,  0.0063,  0.0563,  0.0424,  0.0343, -0.0157, -0.0043,\n",
            "         -0.0175, -0.0123,  0.0013,  0.0568, -0.0275,  0.0545,  0.0325,  0.0194,\n",
            "         -0.0354,  0.0196,  0.0082,  0.0300,  0.0293, -0.0485, -0.0555,  0.0051,\n",
            "         -0.0006,  0.0525,  0.0539, -0.0363,  0.0056, -0.0176, -0.0201, -0.0546,\n",
            "         -0.0064, -0.0423, -0.0379,  0.0070,  0.0212,  0.0058,  0.0550,  0.0540,\n",
            "         -0.0573, -0.0161, -0.0342,  0.0318,  0.0401, -0.0019,  0.0183, -0.0355,\n",
            "         -0.0369,  0.0284,  0.0231, -0.0010, -0.0378, -0.0291,  0.0320, -0.0205,\n",
            "         -0.0270, -0.0114, -0.0246, -0.0333, -0.0136,  0.0443, -0.0426, -0.0411,\n",
            "          0.0209,  0.0154,  0.0229, -0.0313, -0.0085, -0.0084,  0.0128, -0.0120,\n",
            "         -0.0172, -0.0290,  0.0506,  0.0343,  0.0343, -0.0516, -0.0126, -0.0497,\n",
            "          0.0330, -0.0279,  0.0169, -0.0152, -0.0512, -0.0530,  0.0567,  0.0572,\n",
            "          0.0327, -0.0495,  0.0464,  0.0487,  0.0284,  0.0283, -0.0447, -0.0151,\n",
            "         -0.0125, -0.0139,  0.0492, -0.0242, -0.0344,  0.0460,  0.0442, -0.0210,\n",
            "         -0.0181,  0.0449,  0.0222, -0.0177,  0.0059, -0.0025, -0.0057, -0.0312,\n",
            "          0.0477,  0.0380, -0.0037,  0.0175,  0.0444, -0.0040,  0.0439,  0.0163,\n",
            "          0.0571,  0.0170,  0.0062, -0.0207,  0.0046,  0.0483, -0.0462, -0.0452,\n",
            "          0.0327,  0.0078,  0.0341, -0.0519, -0.0340,  0.0041,  0.0202, -0.0301,\n",
            "         -0.0268,  0.0320,  0.0402, -0.0555, -0.0080,  0.0076,  0.0246, -0.0205,\n",
            "          0.0467, -0.0408, -0.0053,  0.0439, -0.0570,  0.0366, -0.0420,  0.0194,\n",
            "          0.0052,  0.0021, -0.0558, -0.0402, -0.0095,  0.0044, -0.0501, -0.0523,\n",
            "          0.0335, -0.0191, -0.0251,  0.0331,  0.0230,  0.0078, -0.0385,  0.0182,\n",
            "         -0.0531,  0.0455, -0.0458, -0.0413,  0.0504,  0.0512, -0.0118, -0.0171,\n",
            "         -0.0087,  0.0288,  0.0124,  0.0225, -0.0482, -0.0340,  0.0452,  0.0489,\n",
            "          0.0549,  0.0503,  0.0257, -0.0136, -0.0036, -0.0076, -0.0031, -0.0307,\n",
            "         -0.0378, -0.0042, -0.0484, -0.0454, -0.0195, -0.0324,  0.0031, -0.0382,\n",
            "          0.0200,  0.0340, -0.0092,  0.0156,  0.0565,  0.0440,  0.0170, -0.0470,\n",
            "          0.0095,  0.0307, -0.0060,  0.0064,  0.0236, -0.0014,  0.0430, -0.0204,\n",
            "          0.0366, -0.0510,  0.0060,  0.0361, -0.0088,  0.0024,  0.0203, -0.0205,\n",
            "         -0.0213,  0.0527, -0.0191, -0.0242,  0.0523, -0.0086, -0.0240, -0.0107,\n",
            "          0.0514,  0.0465, -0.0469, -0.0554, -0.0420,  0.0015, -0.0041, -0.0191,\n",
            "         -0.0160,  0.0448, -0.0154,  0.0011, -0.0507, -0.0267,  0.0455,  0.0088,\n",
            "          0.0034, -0.0328,  0.0292, -0.0495,  0.0356,  0.0187, -0.0350,  0.0283,\n",
            "         -0.0081,  0.0189,  0.0009,  0.0365, -0.0277,  0.0420, -0.0068,  0.0205,\n",
            "          0.0049, -0.0344,  0.0055,  0.0392, -0.0465,  0.0501,  0.0358, -0.0434,\n",
            "          0.0540,  0.0323, -0.0160,  0.0177, -0.0182,  0.0124, -0.0419, -0.0448,\n",
            "         -0.0109,  0.0081,  0.0029,  0.0326]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([10070]) | Values : tensor([-0.0512, -0.0349], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluation"
      ],
      "metadata": {
        "id": "QEywxW7by7L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "learning_rate = 0.0001\n",
        "#batch size has already been defined\n",
        "\n",
        "#loss function for th multi-class clasificiation problem\n",
        "loss_fn = nn.CrossEntropyLoss() #since this is a multi class clasificiation problem we will use Cross Entropy loss\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "lM-927ATy3iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just for the personal refernce to know how many times the loop will work\n",
        "epochs = 5\n",
        "num_batches = len(dataloader)\n",
        "print(num_batches)\n",
        "\n",
        "num_prints_per_epoch = num_batches//100\n",
        "print(f\"{num_prints_per_epoch}\")\n",
        "\n",
        "total_prints  = num_prints_per_epoch * epochs\n",
        "print(f\"{total_prints}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9JgG-Ogy-W8",
        "outputId": "0e98ff4b-6035-4557-c6e3-8d8997527816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "873\n",
            "8\n",
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Forward pass will pass the middle word into the model to get predictions and will calculate loss compared to output and true context word. Whereas, backward pass will update parameters using optimizer.step()***\n"
      ],
      "metadata": {
        "id": "EaF05arN5jf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (center, context) in enumerate(dataloader):\n",
        "        optimizer.zero_grad() #clears previous gradients to prevent accumulation\n",
        "        center = center.long()\n",
        "        context = context.long()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(center)\n",
        "        loss = loss_fn(output, context)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step() #update parameters\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        #will print in gaps of 100 batches\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR3OCQcxzCR7",
        "outputId": "3798b236-2a75-4d58-ebc7-58f70ca52f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Loss: 9.2173\n",
            "Batch 100, Loss: 9.2173\n",
            "Batch 200, Loss: 9.2173\n",
            "Batch 300, Loss: 9.2173\n",
            "Batch 400, Loss: 9.2173\n",
            "Batch 500, Loss: 9.2172\n",
            "Batch 600, Loss: 9.2171\n",
            "Batch 700, Loss: 9.2168\n",
            "Batch 800, Loss: 9.2158\n",
            "Epoch 1/5, Loss: 9.2169\n",
            "Batch 0, Loss: 9.2101\n",
            "Batch 100, Loss: 9.2136\n",
            "Batch 200, Loss: 9.2172\n",
            "Batch 300, Loss: 9.2124\n",
            "Batch 400, Loss: 9.2092\n",
            "Batch 500, Loss: 9.2086\n",
            "Batch 600, Loss: 9.2172\n",
            "Batch 700, Loss: 9.2163\n",
            "Batch 800, Loss: 9.2107\n",
            "Epoch 2/5, Loss: 9.2125\n",
            "Batch 0, Loss: 9.2120\n",
            "Batch 100, Loss: 9.2094\n",
            "Batch 200, Loss: 9.2172\n",
            "Batch 300, Loss: 9.2022\n",
            "Batch 400, Loss: 9.2150\n",
            "Batch 500, Loss: 9.2168\n",
            "Batch 600, Loss: 9.2088\n",
            "Batch 700, Loss: 9.1945\n",
            "Batch 800, Loss: 9.2153\n",
            "Epoch 3/5, Loss: 9.2110\n",
            "Batch 0, Loss: 9.2090\n",
            "Batch 100, Loss: 9.2159\n",
            "Batch 200, Loss: 9.2095\n",
            "Batch 300, Loss: 9.2064\n",
            "Batch 400, Loss: 9.2053\n",
            "Batch 500, Loss: 9.2091\n",
            "Batch 600, Loss: 9.2079\n",
            "Batch 700, Loss: 9.2111\n",
            "Batch 800, Loss: 9.2097\n",
            "Epoch 4/5, Loss: 9.2101\n",
            "Batch 0, Loss: 9.2134\n",
            "Batch 100, Loss: 9.2174\n",
            "Batch 200, Loss: 9.2122\n",
            "Batch 300, Loss: 9.2172\n",
            "Batch 400, Loss: 9.2117\n",
            "Batch 500, Loss: 9.2173\n",
            "Batch 600, Loss: 9.2090\n",
            "Batch 700, Loss: 9.2160\n",
            "Batch 800, Loss: 9.2107\n",
            "Epoch 5/5, Loss: 9.2092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference words"
      ],
      "metadata": {
        "id": "pxWyscL66UBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Using cosine similarity which means if closer to 1  more similar words\n",
        "and if closer to 0  unrelated words ***"
      ],
      "metadata": {
        "id": "vPXnC3pd7dcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "#function to throw a statement if word is not present in vocabulary\n",
        "def find_similar_words(word, top_k=10):\n",
        "    if word not in vocab:\n",
        "        print(\"Word not in vocabulary!\")\n",
        "        return\n",
        "\n",
        "    word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
        "    word_vector = model.embedding(word_idx).detach() #this will be used for word embedding\n",
        "\n",
        "    # use cosine similarity\n",
        "    similarities = []\n",
        "    for i in range(vocab_size):\n",
        "        other_vector = model.embedding(torch.tensor([i], dtype=torch.long)).detach() #embedding every other word in vocab\n",
        "        similarity = F.cosine_similarity(word_vector, other_vector).item()\n",
        "        similarities.append((index2word[i], similarity)) #this will store word and similarity score in similarities list\n",
        "\n",
        "    # Sort by similarity and return top-K words\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "    return [word for word, _ in similarities[:top_k]]\n",
        "\n",
        "#example of vocab not present in dictionary\n",
        "print(find_similar_words(\"religion\", top_k=10))\n",
        "#example 2\n",
        "print(find_similar_words(\"film\", top_k=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUEtWG-yzPDw",
        "outputId": "30d48a0a-14ed-4e32-aa2f-3ec1fdf36948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word not in vocabulary!\n",
            "None\n",
            "['film', 'riverbanks', 'termination', 'yucatn', 'birth', 'marvel', 'rode', 'bring', 'helpful', 'terri']\n"
          ]
        }
      ]
    }
  ]
}